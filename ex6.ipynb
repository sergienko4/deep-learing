{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "ex6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f16b583-d385-41ae-b924-c3dd4396ac37"
      },
      "source": [
        "# Deep Learning: Ex.6 - **Features visualization**\n",
        "\n",
        "Submitted by: [... **your name and ID** ...]\n",
        "\n"
      ],
      "id": "5f16b583-d385-41ae-b924-c3dd4396ac37"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9adbdb5d-8175-486e-84a2-414f81cefac9"
      },
      "source": [
        "# TensorFlow \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PCA and tSNE:\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "print(tf.__version__)"
      ],
      "id": "9adbdb5d-8175-486e-84a2-414f81cefac9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4daf94b8-5cde-4a64-b296-dba082a96da7"
      },
      "source": [
        "---\n",
        "In this question we will use a \"VGG-like\" model (similar to the one you trained in past exercises) that was pre-trained on the CIFAR-10 dataset. you will need to download the model and put it in your working directory.\n",
        "\n",
        "This model consists of the following layers: \n",
        "\n",
        " - input (32x32x3) -> (Conv -> Conv -> Pool) -> (Conv -> Conv -> Pool) -> (Conv -> Conv -> Pool) -> 2048-Dense -> 10-output\n",
        " \n",
        "There are also some batch-normalization and dropout layers in between. All `Conv2D` layers use 3x3 kernels with `padding='same'`.\n",
        "\n",
        "We will use the 10,000 validation images of the CIFAR-10 dataset for the following analysis."
      ],
      "id": "4daf94b8-5cde-4a64-b296-dba082a96da7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7920c0d-8093-4bb9-ab03-a81ae8f707b3"
      },
      "source": [
        "# download the pre-trained model:\n",
        "!git clone https://github.com/rubinj/cifar_model.git\n",
        "\n",
        "# load the model:\n",
        "model = tf.keras.models.load_model('cifar_model/model.h5') \n",
        "\n",
        "for i,l in enumerate(model.layers):\n",
        "    print('%-5i' % i,\n",
        "          '%-20s' % (l.name,),\n",
        "          '%s' % (l.output_shape[1:],))"
      ],
      "id": "d7920c0d-8093-4bb9-ab03-a81ae8f707b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "231e5e47-038a-478b-becd-36e082fc02f0"
      },
      "source": [
        "# download the cifar10 dataset:\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('x_test.shape = ',x_test.shape)\n",
        "print('y_test.shape = ',y_test.shape)"
      ],
      "id": "231e5e47-038a-478b-becd-36e082fc02f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8962702-8935-4185-86d9-5d8d99d16de9"
      },
      "source": [
        "---\n",
        "### 1. Embedding of the feature space in 2-D (using PCA)\n",
        "\n",
        "In this question we will use our pre-trained model as a smart feature-extractor.\n",
        "To this aim, we will use the output from the layer before the last one. This layer produces a 2048-D vector for any given image input of size (32,32,3).\n",
        "\n",
        "- We will use this method, to extract a 2048-D features representation, for each of the 10,000 test images. \n",
        "\n",
        "- Use **PCA** to to reduce the dimensionality of the features **from 2048-D to 2-D**, and use a scatter plot to visualize all samples in this 2-D space. Color the samples by their true label (use the `tab10` color map).\n",
        "\n"
      ],
      "id": "d8962702-8935-4185-86d9-5d8d99d16de9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a64f0be-4459-40b0-b800-fd6972c329f5"
      },
      "source": [
        "    ################################\n",
        "    ###  your code goes here...  ###\n",
        "    ################################"
      ],
      "id": "4a64f0be-4459-40b0-b800-fd6972c329f5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a940cd79-9129-4167-8c3e-de228e824ce5"
      },
      "source": [
        "---\n",
        "### 2. Embedding of the feature space in 2-D (using PCA + tSNE)\n",
        "\n",
        "Usually, tSNE gives better results for this kind of task. The problem is that running tSNE on a large matrix (10,000 x 2,048), can take too long.\n",
        "\n",
        "Therefore, we will first use **PCA** to reduce the dimensionality of the features: **from 2048-D to 50-D**.\n",
        "\n",
        "Then, we will use **tSNE** to further reduce the dimensionality from **50-D to 2-D**.\n",
        "\n",
        "As before, use a scatter plot to visualize all  samples in this 2-D space. Color the samples by their true label (use the `tab10` color map).\n",
        "\n",
        "\n"
      ],
      "id": "a940cd79-9129-4167-8c3e-de228e824ce5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a3418be-3346-407c-8ef2-7aa9cc4ec720"
      },
      "source": [
        "    ################################\n",
        "    ###  your code goes here...  ###\n",
        "    ################################"
      ],
      "id": "6a3418be-3346-407c-8ef2-7aa9cc4ec720",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ba80201-026d-4caf-9ccd-283f116bb4e9"
      },
      "source": [
        "---\n",
        "### 3. Maximally activating patches - `conv2d_1` layer\n",
        "\n",
        "In this question, we will explore the different filters along the pre-trained model.\n",
        "\n",
        "\n",
        "- pick the **2nd conv** layer in the model (`conv2d_1`), and calculate its activation pattern for each and every of the 10,000 images. The result should be a tensor of size: (10000,32,32,32). \n",
        "- pick the **1st filter (channel) in that layer** (out of the 32 available), and look for the neuron with highest activations there (over all 32x32 neurons and 10,000 images). \n",
        "- Print the location `(i,j)` of this neuron and the index number (1-10,000) of the choosen image.\n",
        "- Plot the corresponding patch in that image.\n",
        "\n",
        "hint: `np.argmax` and `np.unravel_index` might come handy.\n"
      ],
      "id": "1ba80201-026d-4caf-9ccd-283f116bb4e9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e79552a8-5c1b-4d41-830f-232aa35dff37"
      },
      "source": [
        "    ################################\n",
        "    ###  your code goes here...  ###\n",
        "    ################################"
      ],
      "id": "e79552a8-5c1b-4d41-830f-232aa35dff37",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKz-Or9kAm8I"
      },
      "source": [
        "- now, instead of finding the single highest activation, find the **8 highest activations** (in a descending order), for the same filter as before. In other words, out of the total (10000,32,32,1) activations find the highest 8.\n",
        "- find the corresponding image patch for each of these 8 activations, and plot them in a single row of subplots."
      ],
      "id": "UKz-Or9kAm8I"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfXAJX5FAnjh"
      },
      "source": [
        "    ################################\n",
        "    ###  your code goes here...  ###\n",
        "    ################################"
      ],
      "id": "TfXAJX5FAnjh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8251135-dca6-42c7-9eb8-051994bbbdd5"
      },
      "source": [
        "- finally, repeat the same process for **9 more filters** (channels) in the same layer (`conv2d_1`): for each of these filters, find the 8 highest activations and extract their corresponding image patches.\n",
        "\n",
        "- plot all the patches you extracted (total 10x8 patches: 10 filters x 8 patches for each one). Use `10x8` subplots."
      ],
      "id": "e8251135-dca6-42c7-9eb8-051994bbbdd5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ab1d087-f20e-4e20-911f-17295d849f79"
      },
      "source": [
        "    ################################\n",
        "    ###  your code goes here...  ###\n",
        "    ################################"
      ],
      "id": "1ab1d087-f20e-4e20-911f-17295d849f79",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b7fc860-71ee-4217-ab67-4f21bad496a9"
      },
      "source": [
        "---\n",
        "### 4. Maximally activating patches - `conv2d_2` layer\n",
        "\n",
        "Repeat the same process for a different layer now: `conv2d_2`.\n",
        "\n",
        "- Extract all activations of that filter: (10000,16,16,64)\n",
        "\n",
        "- Pick 10 filters (out of its 64), and find the 8 highest activations for each filter.\n",
        "\n",
        "- plot all the patches you extracted (total 10x8 patches: 10 filters x 8 patches for each one). Use 10x8 subplots.\n",
        "\n"
      ],
      "id": "3b7fc860-71ee-4217-ab67-4f21bad496a9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40700ec4-20ac-44d2-8802-a1e9eb97e39c"
      },
      "source": [
        "    ################################\n",
        "    ###  your code goes here...  ###\n",
        "    ################################"
      ],
      "id": "40700ec4-20ac-44d2-8802-a1e9eb97e39c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX87a-YcEy5W"
      },
      "source": [
        "---\n",
        "### 5. Maximally activating patches - `conv2d_3` layer\n",
        "\n",
        "Repeat the same process for a different layer now: `conv2d_3`.\n",
        "\n",
        "- Extract all activations of that filter: (10000,16,16,64)\n",
        "\n",
        "- Pick 10 filters (out of its 64), and find the 8 highest activations for each filter.\n",
        "\n",
        "- plot all the patches you extracted (total 10x8 patches: 10 filters x 8 patches for each one). Use 10x8 subplots.\n",
        "\n",
        "\n",
        "Pay careful attention to the way you transform the index `(i,j)` in the activation layer to the correct patch in the image.. \n",
        "\n",
        "You should get results similar to the ones in the presentation slides."
      ],
      "id": "rX87a-YcEy5W"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjSVBHHGEzJa"
      },
      "source": [
        "    ################################\n",
        "    ###  your code goes here...  ###\n",
        "    ################################"
      ],
      "id": "wjSVBHHGEzJa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03285cd2-e01a-4faa-aafc-0c2689aeebff"
      },
      "source": [
        "***\n",
        "## Good Luck!"
      ],
      "id": "03285cd2-e01a-4faa-aafc-0c2689aeebff"
    }
  ]
}