{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning: Ex.4 - Training Networks\n",
    "\n",
    "Submitted by: [... **name & ID** ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dropout, BatchNormalization  # <-- new layers!\n",
    "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom, RandomTranslation # <-- new layers!\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from seaborn import heatmap \n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the CIFAR-10 Dataset\n",
    "\n",
    "We will use the same CIFAR-10 dataset as in Ex.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. load/download the data\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# 2. flatten the labels (easier to deal with)\n",
    "train_labels = train_labels.flatten()  # (50000, 1) -> (50000,)\n",
    "test_labels = test_labels.flatten()    # (10000, 1) -> (10000,)\n",
    "\n",
    "# 3. convert uint8->float32 and normalize range to 0.0-1.0 \n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# 4. define the 10 classes names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer','dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# 5. print the shapes\n",
    "print('train_images.shape =',train_images.shape)\n",
    "print('train_labels.shape =',train_labels.shape)\n",
    "print('test_images.shape =',test_images.shape)\n",
    "print('test_labels.shape =',test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 1. Testing SGD options\n",
    "\n",
    "Our basic model will be the same as in Ex.3:\n",
    "\n",
    "- `Input` layer\n",
    "- 32 3x3-`Conv2D` + 2x2 `MaxPooling` \n",
    "- 64 3x3-`Conv2D` + 2x2 `MaxPooling` \n",
    "- 128 3x3-`Conv2D` + 2x2 `MaxPooling` \n",
    "- 128-`Dense` \n",
    "- 10-`Dense` - output layer \n",
    "\n",
    "\n",
    "Prepeare a function that returns this model (without the `compile` part, just the layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_basic_model():\n",
    "    # build out basic model\n",
    "    the_model = Sequential()\n",
    "    the_model.add(Input(shape=(32,32,3)))\n",
    "    the_model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "    the_model.add(MaxPooling2D((2,2)))\n",
    "    the_model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "    the_model.add(MaxPooling2D((2,2)))\n",
    "    the_model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "    the_model.add(MaxPooling2D((2,2)))\n",
    "    the_model.add(Flatten())\n",
    "    the_model.add(Dense(128, activation='relu'))\n",
    "    the_model.add(Dense(10, activation='softmax'))\n",
    "    return the_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will train the same model, each time using a different optimizer:\n",
    "- SGD with `learning_rate = 0.01`  (the default value)\n",
    "- SGD with `learning_rate = 0.001` \n",
    "- SGD with `learning_rate = 0.1`\n",
    "- SGD with `learning_rate = 0.01` and `momentum = 0.9`\n",
    "\n",
    "\n",
    "In order to train each model from scratch (and not to continue training the same model again and again), create a new model each time (m1, m2, m3, m4).\n",
    "\n",
    "Also, use a **different variable** to record the `history` of the training results (h1, h2, h3, h4).\n",
    "\n",
    "Train each model for 100 epochs with a batch size of 64 (remember to use a GPU), and plot the usual graphs (loss&accuracy for train&test).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  SGD with learning_rate = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ################################\n",
    "        ###  your code goes here...  ###\n",
    "        ################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD with learning_rate = 0.001 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ################################\n",
    "        ###  your code goes here...  ###\n",
    "        ################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD with learning_rate = 0.1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ################################\n",
    "        ###  your code goes here...  ###\n",
    "        ################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD with learning_rate = 0.01 and momentum = 0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ################################\n",
    "        ###  your code goes here...  ###\n",
    "        ################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphical comparison \n",
    "\n",
    "Finally, In a single graph, plot together the training loss curves of all 4 runs (use different color for each plot, and add a proper legend).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ################################\n",
    "        ###  your code goes here...  ###\n",
    "        ################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Add Dropout\n",
    "\n",
    "In order to overcome the over-training we will add `dropout` layers: one `dropout` layer before each of the `Dense` layers. Use a 20% dropout rate.\n",
    "\n",
    "Pick your favorite SGD optimizer and train the network for 50 Epochs. \n",
    "\n",
    "- Verify that you get better results in terms of over-training (less over-training is better..)\n",
    "\n",
    "- Did you also get a better accuracy on the validation data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ################################\n",
    "        ###  your code goes here...  ###\n",
    "        ################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Add Data Augmentation\n",
    "\n",
    "Add 2-3 layers of data augmentation (of your choice) to the previous model (with the dropout).\n",
    "\n",
    "- Train the model (50 epoch)\n",
    "\n",
    "- Did you get better results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ################################\n",
    "        ###  your code goes here...  ###\n",
    "        ################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Deeper model\n",
    "\n",
    "Finally, lets make the model a bit deeper, doubling each `Conv2D` layer.\n",
    "We will also add `BatchNormalization` layers in between, to help the learning converge faster\n",
    "\n",
    "- 32 3x3-`Conv2D` + BatchNormalization()` + 32 3x3-`Conv2D` + BatchNormalization()` + 2x2 `MaxPooling` \n",
    "- 64 3x3-`Conv2D` + BatchNormalization()` + 64 3x3-`Conv2D` + BatchNormalization()` + 2x2 `MaxPooling` \n",
    "- 128 3x3-`Conv2D` + BatchNormalization()` + 128 3x3-`Conv2D` + BatchNormalization()` + 2x2 `MaxPooling` \n",
    "\n",
    "Train this model (including the data augmentation and drop-out) and plot the usual graphs.\n",
    "\n",
    "(Hopefully, you should get close to 90% accuracy..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ################################\n",
    "        ###  your code goes here...  ###\n",
    "        ################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Good Luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
