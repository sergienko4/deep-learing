{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "ex5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_qqEIFSv_s2"
      },
      "source": [
        "# Deep Learning: Ex.5 - **Transfer Learning**\n",
        "\n",
        "Submitted by: [... **your name and ID** ...]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbo035pbv_s5"
      },
      "source": [
        "# TensorFlow \n",
        "import tensorflow as tf\n",
        "# (import more modules if needed...)\n",
        "\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4wDfwfJwlLb"
      },
      "source": [
        "---\n",
        "### loading the `EuroSAT` dataset\n",
        "\n",
        "- for more info on this dataset see: https://github.com/phelber/eurosat \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILe0rF9Gwj5N"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = tfds.as_numpy(tfds.load(\n",
        "    'eurosat',\n",
        "    split=['train[:80%]', 'train[80%:]'], \n",
        "    batch_size = -1,\n",
        "    shuffle_files = True,\n",
        "    as_supervised = True))\n",
        "\n",
        "LABELS = [\n",
        "'AnnualCrop', 'Forest', 'Vegetation', 'Highway', 'Industrial',\n",
        "'Pasture', 'PermCrop', 'Residential', 'River', 'SeaLake']\n",
        "\n",
        "train_data = train_data.astype('float32') / 255.0\n",
        "train_labels = train_labels.flatten()\n",
        "\n",
        "test_data = test_data.astype('float32') / 255.0\n",
        "test_labels = test_labels.flatten()\n",
        "\n",
        "[idx] = np.where(train_labels==5) \n",
        "plt.figure(figsize=(20,12))\n",
        "for c in range(10):\n",
        "    [idx] = np.where(train_labels == c) \n",
        "    for i in range(15):\n",
        "        plt.subplot(10,15,c*15+i+1)\n",
        "        plt.imshow(train_data[idx[i]])\n",
        "        plt.xticks([]), plt.yticks([])\n",
        "        if i==0:\n",
        "            plt.ylabel(LABELS[c])\n",
        "plt.show()\n",
        "\n",
        "print('train_data.shape =',train_data.shape)\n",
        "print('test_data.shape =',test_data.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muATKBGgyi0d"
      },
      "source": [
        "---\n",
        "### 1. Data augmentation\n",
        "\n",
        "Since our dataset is rather small, we will use data augmentation to avoid overfitting.\n",
        "\n",
        "- Choose a set of suitable preprocessing layers (RandomFlip, RandomRotation,...)\n",
        "- Pick a single training image and apply your preprocessing set on it, to generate 36 new variations from it.\n",
        "- Display the 36 variations using a 6x6 subplots. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jr8bg4Vz0l6"
      },
      "source": [
        "    ################################\n",
        "    ###  your code goes here...  ###\n",
        "    ################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z74oc5nyv_s7"
      },
      "source": [
        "***\n",
        "### 2. Use a pretrained VGG16 model\n",
        "\n",
        "Load the pre-trained VGG16 model without its \"classifier top\" (= without the dense layers).\n",
        "\n",
        "- Create a new model, with the data augmentation layers first, then the VGG pre-trained layers, and finally add a new classification head to match our task.\n",
        "\n",
        "- print your model `summary`. How many trainable parameters you have?\n",
        "\n",
        "- freeze the layer in your model, corresponding to the pre-trained VGG weights.\n",
        "\n",
        "- re-print your model `summary`. Verify that you have much fewer trainable parameters (how many?).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKF0AFtev_s7"
      },
      "source": [
        "    ################################\n",
        "    ###  your code goes here...  ###\n",
        "    ################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slPbmZznv_s8"
      },
      "source": [
        "***\n",
        "### 3. Transfer learning\n",
        "\n",
        "We will traom the weights of the new \"top\":\n",
        "\n",
        "Train the new model (with the frozen VGG layers) for ~30 epochs and plot the usual graphs (including a confusion matrix).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxL_rMGbv_s9"
      },
      "source": [
        "    ################################\n",
        "    ###  your code goes here...  ###\n",
        "    ################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AapEtpYLv_s9"
      },
      "source": [
        "***\n",
        "### 4. Fine tuning\n",
        "\n",
        "At this point, 'unfreeze' the VGG layers and continue the training process (with slower learning rate) to get better results.\n",
        "\n",
        "Train the unfrozen model for ~50 more epochs.\n",
        "\n",
        "- use: `Adam(learning_rate=0.00001)` \n",
        "\n",
        "Remember to recompile your model, in order to the changes to take effect (print `model.summary()` to verify).\n",
        "\n",
        "- Plot the usual graphs, **but** this time add **together** the new 50 epochs to the previous 30 epochs (see the last slide from the lecture)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVCoPZSiv_s-"
      },
      "source": [
        "    ################################\n",
        "    ###  your code goes here...  ###\n",
        "    ################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efYlFPR2v_s-"
      },
      "source": [
        "***\n",
        "## Good Luck!"
      ]
    }
  ]
}